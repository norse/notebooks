{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7IYU0Bqomb2"
   },
   "source": [
    "# Training a classifier on the event-based POKER-DVS dataset\n",
    "\n",
    "When working with Spiking Neural Networks (SNN), we will inevitably encounter the notion of _time_ in our network and data flow. The classic example of MNIST handwritten digits consists of images, much like snapshots in time. Deep learning has shown impressive results on such purely spatial compositions, but SNNs might be able to extract meaning from temporal features and/or save power doing so in comparison to classical networks. \n",
    "\n",
    "An event camera such as the Dynamic Vision Sensor (DVS) is [somewhat based](https://medium.com/@gregorlenz/rethinking-the-way-our-cameras-see-8584b5167bb) on the functional principle of the human retina. Such a camera can record a scene much more efficiently than a conventional camera by encoding the changes in a visual scene rather than absolute illuminance values. The output is a spike train of change detection events for each pixel. While previously we had to use encoders to equip static image data with a temporal dimension, the POKER-DVS dataset contains recordings of poker cards that are shown to an event camera in rapid succession.\n",
    "\n",
    "**Warning!** This notebook uses a large dataset and can take a significant amount of time to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wu93JGgT2CJ2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rmUJSdzqypr"
   },
   "source": [
    "We can simply install Norse through pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DPb7tCeX2Jkb",
    "outputId": "27c75437-737b-43f3-eb76-10ec7e7983ad"
   },
   "outputs": [],
   "source": [
    "!pip install norse --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial we are going to make use of a package that handles event-based datasets called [Tonic](https://github.com/neuromorphs/tonic). It is based on PyTorch Vision, so you should already have most of its dependencies installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tonic --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading the POKER-DVS dataset and specifying a sparse tensor transform whenever a new sample is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "ZzPRyc8E2M8a",
    "outputId": "7623e613-923f-451a-f8bf-f1c05ad72912"
   },
   "outputs": [],
   "source": [
    "import tonic\n",
    "import torchvision\n",
    "\n",
    "sensor_size = tonic.datasets.POKERDVS.sensor_size\n",
    "frame_transform = tonic.transforms.ToFrame(sensor_size=sensor_size, time_window=1000)\n",
    "\n",
    "trainset = tonic.datasets.POKERDVS(save_to='./data', train=True)\n",
    "testset = tonic.datasets.POKERDVS(save_to='./data', transform=frame_transform, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look at how a sample of one digit looks like. The event camera's output is encoded as events that have x/y coordinates, a timestamp and a polarity that indicates whether the lighting increased or decreased at that event. The events are provided in an (NxE) array. Let's have a look at the first example in the dataset. Every row in the array represents one event of timestamp, x, y, and polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = trainset[0][0]\n",
    "events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When accumulated over time into 3 bins, the images show 1 of 4 card symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tonic.utils.plot_event_grid(events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this one is the target class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrap the training and testing sets in PyTorch DataLoaders that facilitate file loading. Note also the custom collate function __pad_tensors__ , which makes sure that all sparse tensors in the batch have the same dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420,
     "referenced_widgets": [
      "a39cccabae994485b9b3f0866d6f1891",
      "48509a5c085441f29d908f03e17104f1",
      "49e4961d66cf49d096fe542250bd7ea4",
      "699927de0a9a44438c7f77f53c80ca41",
      "be29cb04ef804c31acbe5b57a4254f8e",
      "18f3adf705754ed486b4860c38a443a9",
      "3313b75ccbea4d1f89409d4abb47b8f6",
      "d1268b0592cd4f9a9a4c04c939dc0402",
      "999ca0343fbd4693bfeb6e75df29aaa4",
      "2537d663e9a941cdb3239daa8f463145",
      "06c79d8b76d646f597091e64b3ac8a7c",
      "adf7256611d04523a8d77a6c0079db52",
      "34055b8bbea048378f7e71a498cba0f8",
      "eb3b3c6cf9024c7899fbc7f588dec2e0",
      "d3b69d17635c4be09e1e66d8bf3c190f",
      "700180bf289944e4898a6ea9bb7e6815",
      "b38e0b3b846d44f08a68884a6ec894eb",
      "83d347927a6147f8ae69818bfa66fd88",
      "58a275f42e63442fa5306f85ee09e094",
      "77fd86441e54485dad7dfee7b3b26d2b",
      "89a9086e97834567b65d188efdd0fe66",
      "dae7cfd7c37f4278b661c9fd14de3aed",
      "bbd4e89f0ed0472f83ebe824f6177020",
      "256419cbcfde4ee8bfe6fc35beb236ee",
      "d4a6f8b8cfc34899a809581d3f10b414",
      "40735b0849ad472b893f465085013963",
      "0f11348c44cc48a98135e4e5d2d5de73",
      "d1aa16b66d21488d93b9b54141190798",
      "23235a7e41c547f49395137b57387d69",
      "a4c87414f66747bb9532195c6569364b",
      "b33e85cc5a304bd798bbd294bae3e236",
      "50891ba2e81542eeba75ed311f1398ca"
     ]
    },
    "id": "RPs81D-QrFWV",
    "outputId": "0eb067e3-fc00-486d-d618-7ca41bd70535"
   },
   "outputs": [],
   "source": [
    "# reduce this number if you run out of GPU memory\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# add sparse transform to trainset, previously omitted because we wanted to look at raw events\n",
    "trainset.transform = frame_transform\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        collate_fn=tonic.collation.PadTensors(batch_first=False),\n",
    "                                        shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(testset,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        collate_fn=tonic.collation.PadTensors(batch_first=False),\n",
    "                                        shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNEqcSNH2WfP"
   },
   "source": [
    "## Defining a Network\n",
    "\n",
    "Once the data is encoded into spikes, a spiking neural network can be constructed in the same way as a one would construct a recurrent neural network.\n",
    "Here we define a spiking neural network with one recurrently connected layer\n",
    "with `hidden_features` LIF neurons and a readout layer with `output_features` and leaky-integrators. As you can see, we can freely combine spiking neural network primitives with ordinary `torch.nn.Module` layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qN9Sm4rJtgc7"
   },
   "outputs": [],
   "source": [
    "from norse.torch import LIFParameters, LIFState\n",
    "from norse.torch.module.lif import LIFCell, LIFRecurrentCell\n",
    "# Notice the difference between \"LIF\" (leaky integrate-and-fire) and \"LI\" (leaky integrator)\n",
    "from norse.torch import LICell, LIState\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "class SNNState(NamedTuple):\n",
    "    lif0 : LIFState\n",
    "    readout : LIState\n",
    "\n",
    "\n",
    "class SNN(torch.nn.Module):\n",
    "    def __init__(self, input_features, hidden_features, output_features, tau_syn_inv, tau_mem_inv, record=False, dt=1e-3):\n",
    "        super(SNN, self).__init__()\n",
    "        self.l1 = LIFRecurrentCell(\n",
    "            input_features,\n",
    "            hidden_features,\n",
    "            p=LIFParameters(alpha=100, \n",
    "                            v_th=torch.as_tensor(0.3),\n",
    "                            tau_syn_inv=tau_syn_inv,\n",
    "                            tau_mem_inv=tau_mem_inv,\n",
    "                           ),\n",
    "            dt=dt                     \n",
    "        )\n",
    "        self.input_features = input_features\n",
    "        self.fc_out = torch.nn.Linear(hidden_features, output_features, bias=False)\n",
    "        self.out = LICell(dt=dt)\n",
    "\n",
    "        self.hidden_features = hidden_features\n",
    "        self.output_features = output_features\n",
    "        self.record = record\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_length, batch_size, _, _, _ = x.shape\n",
    "        s1 = so = None\n",
    "        voltages = []\n",
    "\n",
    "        if self.record:\n",
    "            self.recording = SNNState(\n",
    "              LIFState(\n",
    "                z = torch.zeros(seq_length, batch_size, self.hidden_features),\n",
    "                v = torch.zeros(seq_length, batch_size, self.hidden_features),\n",
    "                i = torch.zeros(seq_length, batch_size, self.hidden_features)\n",
    "              ),\n",
    "              LIState(\n",
    "                v = torch.zeros(seq_length, batch_size, self.output_features),\n",
    "                i = torch.zeros(seq_length, batch_size, self.output_features)\n",
    "              )\n",
    "            )\n",
    "\n",
    "        for ts in range(seq_length):\n",
    "            z = x[ts, :, :, :].view(-1, self.input_features)\n",
    "            z, s1 = self.l1(z, s1)\n",
    "            z = self.fc_out(z)\n",
    "            vo, so = self.out(z, so)\n",
    "            if self.record:\n",
    "                self.recording.lif0.z[ts,:] = s1.z\n",
    "                self.recording.lif0.v[ts,:] = s1.v\n",
    "                self.recording.lif0.i[ts,:] = s1.i\n",
    "                self.recording.readout.v[ts,:] = so.v\n",
    "                self.recording.readout.i[ts,:] = so.i\n",
    "            voltages += [vo]\n",
    "\n",
    "        return torch.stack(voltages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a good idea to test the network's response to time constant parameters that depend on the duration of recordings in the dataset as well as average number of events. We use dt=1e-6 because the events we're dealing with have microsecond resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_snn = SNN(np.product(trainset.sensor_size), 100, len(trainset.classes), tau_syn_inv=torch.tensor(1/1e-2), tau_mem_inv=torch.tensor(1/1e-2), record=True, dt=1e-3)\n",
    "\n",
    "frames, target = next(iter(train_loader))\n",
    "\n",
    "frames[:,:1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are only applying a subset (`1000`) of the data timesteps (`22227`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_readout_voltages = example_snn(frames[:,:1])\n",
    "voltages = example_readout_voltages.squeeze(1).detach().numpy()\n",
    "\n",
    "plt.plot(voltages)\n",
    "plt.ylabel('Voltage [a.u.]')\n",
    "plt.xlabel('Time [us]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(example_snn.recording.lif0.v.squeeze(1).detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(example_snn.recording.lif0.i.squeeze(1).detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1jcJ7LnrlUi"
   },
   "source": [
    "## Training the Network\n",
    "\n",
    "The final model is then simply the sequential composition of our network and a decoding step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(x):\n",
    "    x, _ = torch.max(x, 0)\n",
    "    log_p_y = torch.nn.functional.log_softmax(x, dim=1)\n",
    "    return log_p_y\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, snn, decoder):\n",
    "        super(Model, self).__init__()\n",
    "        self.snn = snn\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.snn(x)\n",
    "        log_p_y = self.decoder(x)\n",
    "        return log_p_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C4QeDXL9_qaB",
    "outputId": "533ce0f5-33fe-4cde-897b-218be6d43b9f"
   },
   "outputs": [],
   "source": [
    "LR = 0.002\n",
    "INPUT_FEATURES = np.product(trainset.sensor_size)\n",
    "HIDDEN_FEATURES = 100\n",
    "OUTPUT_FEATURES = len(trainset.classes)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "model = Model(\n",
    "    snn=SNN(\n",
    "      input_features=INPUT_FEATURES,\n",
    "      hidden_features=HIDDEN_FEATURES,\n",
    "      output_features=OUTPUT_FEATURES,\n",
    "      tau_syn_inv=torch.tensor(1/1e-2), \n",
    "      tau_mem_inv=torch.tensor(1/1e-2)\n",
    "    ),\n",
    "    decoder=decode\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rM5btRjKdEEv"
   },
   "source": [
    "What remains to do is to setup training and test code. This code is completely independent of the fact that we are training a spiking neural network and in fact has been largely copied from the pytorch tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SXgntmL_rvHO"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for (data, target) in tqdm(train_loader, leave=False):\n",
    "        data, target = data.to(device), torch.LongTensor(target).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = torch.nn.functional.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    mean_loss = np.mean(losses)\n",
    "    return losses, mean_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtdcQi_18Xip"
   },
   "source": [
    "Just like the training function, the test function is standard boilerplate, common with any other supervised learning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gca4ZzatApWD"
   },
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), torch.LongTensor(target).to(device)\n",
    "            output = model(data)\n",
    "            test_loss += torch.nn.functional.nll_loss(\n",
    "                output, target, reduction=\"sum\"\n",
    "            ).item()  # sum up batch loss\n",
    "            pred = output.argmax(\n",
    "                dim=1, keepdim=True\n",
    "            )  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    accuracy = 100.0 * correct / len(test_loader.dataset)\n",
    "\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "83471770562e4dc6a9c0203567a11738",
      "79a47e5c73884eaa95aa3cf1798bd34e",
      "e8815d2923f64385993f68ba2a0ff83a",
      "c9dddaf73fac46d78a2616763c3498ff",
      "a51b7d3fd61d450c89bc5525ce31c495",
      "9904c1cbdddf479a8491807d3810b4a0",
      "c65e235f0f21418b80d8df61fcea0229",
      "1cec1ba4c03342f29d6fa3bac1fdef79",
      "0db2860f929b4728a58a5a60a76d0661",
      "afe0f290c061434a9868f4f7884ecae4",
      "5a4f2f4068e3463fbcec7a8cfcc6d99c",
      "a4cbd02d8b47464a8fefab8bd4be1a4f",
      "1c76c1d0a7a443b89e34a6606b5db3e2",
      "6ffc4204c106450c82577624db006d7d",
      "ecce7c70c4bb41cb9984529e1ea836d9",
      "6a0653febe6443768d9f04693a6815dc",
      "ba0526a00832430696c50c3c3990ea52",
      "37ff024648ef4a328ab218827d1e5e8a",
      "f2581d6f8e014b7f8c47c9748966ddc5",
      "5f5c829842734e109a062d7822308738",
      "d307e64e0add463e8ec0fb8556c02532",
      "63d12d053b194af5a064c23916544fe6",
      "fdcd12c58ce34ac9a1d64132e9dfaade",
      "528b36c345eb4c16a043b3fcb0d26a02",
      "f82b17289112486fa1340b5d7dd14c0e",
      "dc0e7177ec804b0fb99b85d56eaf3ad5",
      "4f77245233e54f01a23d2070d049f63d",
      "ec712d288cee4f1eb2bcf6af2a2e3673",
      "ae87af3bb7014b19b0dd0c515a115724",
      "ee324024183546d5b5e563ceb5bcbd6e",
      "71f7be117e3840ed8ecd71c30b92ccd1",
      "0d985ad92a734753923318035caee6d8",
      "4e9e3649f45a4fd78cff0efb1a776c8d",
      "08d8223656ea4402851911a5b8b320b6",
      "120e2e6d1209454bac8a77dba77a3181",
      "983c4760eb4247f2b053b70fa8c470f8",
      "235c4ed216ef4bebbe5cbb45e5e8a432",
      "4e81dcf2a69e4255b68f96594a5d8f46",
      "e3233a8ac4b548b78d62f5312a624b39",
      "4690821d1e4744ce8a1e00bd92f612dd",
      "a01d369fee314a3390ee34d4c7c27c63",
      "21d8540ae0a5444f8cdca7d00d3b3034",
      "7bab9f7eead449d6b2f043fddeaeaa5f",
      "ef6ff21e1b8843939c97d844d8245330",
      "f239ddb5cf8a491d857d4e1bbcb1bc06",
      "cd0f7ad4e21f4b2580f9dd0c8d333586",
      "5adc970dab4346a9a54afbeeb81c91e0",
      "34988d7cfbbb42aeac37b7dfe5fc1288"
     ]
    },
    "id": "wU-b7Q8eBVca",
    "outputId": "5f992877-857f-4630-c9ae-78ee999679bf"
   },
   "outputs": [],
   "source": [
    "training_losses = []\n",
    "mean_losses = []\n",
    "test_losses = []\n",
    "accuracies = []\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "EPOCHS  = 10\n",
    "\n",
    "for epoch in trange(EPOCHS):\n",
    "    training_loss, mean_loss = train(model, DEVICE, train_loader, optimizer, epoch)\n",
    "    test_loss, accuracy = test(model, DEVICE, test_loader, epoch)\n",
    "    training_losses += training_loss\n",
    "    mean_losses.append(mean_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "print(f\"final accuracy: {accuracies[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKEVGF76x_Ee"
   },
   "source": [
    "We can visualize the output of the trained network on an example input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "NL_rtLC2xXLp",
    "outputId": "f6d08cf8-d3f1-4636-f190-ed6421d29256"
   },
   "outputs": [],
   "source": [
    "trained_snn = model.snn\n",
    "trained_readout_voltages = trained_snn(frames[:,:1].to(\"cuda\"))\n",
    "plt.plot(trained_readout_voltages.squeeze(1).cpu().detach().numpy())\n",
    "\n",
    "plt.ylabel('Voltage [a.u.]')\n",
    "plt.xlabel('Time [ms]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "norse-tutorial.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
