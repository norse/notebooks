{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Computing\n",
    "\n",
    "The goal of this part of the workshop is to explore a simple example \n",
    "of stochastic computing with spiking neurons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Install Requirements\n",
    "\n",
    "First of all, we will need to install Norse. Please run the cell below. Read on while it's running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install norse --quiet\n",
    "    \n",
    "import torch\n",
    "import norse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Neuron Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "from norse.torch.module import LIFRefracRecurrentCell\n",
    "from norse.torch.functional.threshold import superspike_fn\n",
    "from norse.torch.functional.lif_refrac import lif_refrac_feed_forward_step, lif_refrac_step\n",
    "from norse.torch.functional.lif_refrac import LIFRefracFeedForwardState, LIFRefracState, LIFRefracParameters\n",
    "from norse.torch.functional.lif import LIFFeedForwardState, LIFState, LIFParameters\n",
    "from norse.torch.functional.encode import poisson_encode\n",
    "\n",
    "\n",
    "class LIFRefracNeurons(torch.nn.Module):\n",
    "  def __init__(self, K, N):\n",
    "    super(LIFRefracNeurons, self).__init__()\n",
    "\n",
    "    self.K = K\n",
    "    self.N = N\n",
    "\n",
    "    self.v_leak = torch.nn.Parameter(1.0*torch.ones(N, device=device), requires_grad=False)\n",
    "    self.rho = torch.nn.Parameter(10.0*torch.ones(N, device=device), requires_grad=False)\n",
    "    lif_parameter = LIFParameters(v_leak=self.v_leak, method=\"super\", alpha=torch.tensor(100))\n",
    "    lif_refrac = LIFRefracParameters(lif_parameter, self.rho)\n",
    "    self.cell = LIFRefracRecurrentCell(K, N, p=lif_refrac)\n",
    "\n",
    "  def forward(self, z):\n",
    "    T, B, _ = z.shape\n",
    "    s0 = LIFRefracState(LIFState(torch.zeros(B,self.N, device=z.device), 0.9*torch.ones(B,self.N, device=z.device), torch.zeros(B,N, device=z.device)), torch.zeros(B, self.N, device=z.device))\n",
    "    refrac = []\n",
    "    voltages = []\n",
    "\n",
    "    for ts in range(T):\n",
    "      output, s0 = self.cell(z[ts,:], s0)\n",
    "      r = superspike_fn(s0.rho, torch.tensor(100.0, device=z.device))\n",
    "      refrac.append(r)\n",
    "      voltages.append(s0.lif.v)\n",
    "    return torch.stack(refrac), torch.stack(voltages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Task\n",
    "\n",
    "The goal will be to approximate a binary probability distribution on three binary variables\n",
    "$z_1, z_2, z_3$ represented by the refractory state $\\rho$ of three neurons. The correlation between the three refractory state variables can be computed in the following way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_point_correlations_func(rho):\n",
    "  T, B, _ = rho.shape\n",
    "\n",
    "  rb0 = torch.stack([1-rho[:,:,0], rho[:,:,0]])\n",
    "  rb1 = torch.stack([1-rho[:,:,1], rho[:,:,1]])\n",
    "  rb2 = torch.stack([1-rho[:,:,2], rho[:,:,2]])\n",
    "  return torch.einsum(\"atb,dtb,gtb -> badg\", rb0, rb1, rb2) / T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question (Optional): Can you think of a way how to implement this for an \n",
    "arbitrary number of neurons?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate the target distribution by sampling from a population of refractory\n",
    "neurons, that way we can be reasonably sure that the model can be trained quickly\n",
    "to approximate the target distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_target_distribution(timesteps, batch_dimension, input_features):\n",
    "    T, B, H = timesteps, batch_dimension, input_features\n",
    "    z = poisson_encode(0.3 * torch.ones(B, H), T).detach().to(device)\n",
    "    n_target = LIFRefracNeurons(H,N).to(device)\n",
    "    refrac_t, voltages_t = n_target(z)\n",
    "    readout_t = refrac_t[:,:,:5]\n",
    "    p_target = three_point_correlations_func(readout_t).detach()\n",
    "    p_target = p_target.flatten(start_dim=1)[0].unsqueeze(0).repeat(B,1)\n",
    "    return p_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the target we can use the following helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def target_plot(ax, p_target, width=0.35):\n",
    "  ax.bar(np.arange(0,8), p_target[0], width, label='$p_{t}$')\n",
    "  ax.legend(loc='upper right')\n",
    "  ax.set_xticks(np.arange(0,8))\n",
    "  ax.set_xticklabels(['{0:03b}'.format(n) for n in np.arange(0,8)], rotation = 90)\n",
    "  ax.set_ylabel('$p(z)$')\n",
    "  ax.set_xlabel('$z$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate a target distribution.\n",
    "\n",
    "Once you've run the notebook to completion, you can\n",
    "consider doing one of the following Tasks: \n",
    "\n",
    "- Vary the number of timesteps, how to you expect fidelity of the approximation to behave?\n",
    "- Change the number of visible units (N) and number of poisson noise sources (H), what do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 32 # number of visible units\n",
    "H = 256 # number of synapses connected to poisson noise sources\n",
    "B = 32 # batch dimension\n",
    "T = 1000 # number of timesteps to integrate (increase to get better sampling accuracy)\n",
    "rho = 20 # refractory time in timesteps\n",
    "device = \"cpu\"\n",
    "\n",
    "p_target = generate_target_distribution(\n",
    "    timesteps=T,\n",
    "    batch_dimension=B, \n",
    "    input_features=H\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "target_plot(ax, p_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Training\n",
    "\n",
    "The goal of the training procedure will be to minimize the [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullbackâ€“Leibler_divergence) between the target distribution $p_t$ and the batch-averaged sampled distribution $p_s$. In the machine-learning language the Kullback-Leibler divergence is one example of a loss-function and luckily PyTorch provides an [implementation](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html). This is another good example where relying on a widely used library saves us a lot of implementation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange\n",
    "\n",
    "neurons = LIFRefracNeurons(H,N).to(device)\n",
    "optim = torch.optim.Adam(neurons.parameters())\n",
    "losses = []\n",
    "\n",
    "epochs = 40 # number of epochs to optimize\n",
    "pbar = trange(epochs)\n",
    "\n",
    "for i in pbar:\n",
    "  optim.zero_grad()\n",
    "\n",
    "  # sample new poisson noise for every iteration\n",
    "  z = poisson_encode(0.3 * torch.ones(B, H), T).detach().to(device)\n",
    "\n",
    "  # compute the refractory state and voltage traces\n",
    "  refrac, voltages = neurons(z)\n",
    "\n",
    "  # the first three neurons in each batch are our readout neurons\n",
    "  readout = refrac[:,:,:3]\n",
    "\n",
    "  # compute sampled probability distribution\n",
    "  p_f = three_point_correlations_func(refrac).flatten(start_dim=1)\n",
    "\n",
    "  # batch averaged Kullback-Leibler divergence \n",
    "  loss = torch.nn.functional.kl_div(torch.log(p_f.clamp(min=1e-7)), p_target, reduction='batchmean')\n",
    "\n",
    "  # propagate gradient to parameters through time and take optimisation step\n",
    "  loss.backward()\n",
    "  optim.step()\n",
    "\n",
    "  pbar.set_postfix({\"loss\": loss.detach().item()})\n",
    "  losses.append(loss.detach().item())\n",
    "\n",
    "\n",
    "# save all the training results\n",
    "basepath = \"results/stochastic\"\n",
    "os.makedirs(basepath, exist_ok=True)\n",
    "np.save(os.path.join(basepath, \"refrac.npy\"), refrac.detach().numpy())\n",
    "np.save(os.path.join(basepath, \"p_target.npy\"), p_target.detach().numpy())\n",
    "np.save(os.path.join(basepath, \"p_f.npy\"), p_f.detach().numpy())\n",
    "np.save(os.path.join(basepath, \"losses.npy\"), np.stack(losses))\n",
    "np.save(os.path.join(basepath, \"voltages.npy\"), voltages.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model should converge to a loss of ~0.02-0.04.\n",
    "\n",
    "Tasks / Questions:\n",
    "- What could be done to improve upon this result?\n",
    "- Change the input poisson noise, what happens?\n",
    "- Find out about different [optimisers](https://pytorch.org/docs/stable/optim.html) and try another one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluating the Result\n",
    "\n",
    "Every scientist knows that a great plot is more than half of the reward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = \"results/stochastic\"\n",
    "\n",
    "refrac = torch.tensor(np.load(os.path.join(basepath, \"refrac.npy\")))\n",
    "p_target = torch.tensor(np.load(os.path.join(basepath, \"p_target.npy\")))\n",
    "p_f = torch.tensor(np.load(os.path.join(basepath, \"p_f.npy\")))\n",
    "losses = torch.tensor(np.load(os.path.join(basepath, \"losses.npy\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is not really part of the workshop, but is necessary to\n",
    "create a decent (enough) looking figure of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import PatchCollection, LineCollection\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def negedge(z, z_prev):\n",
    "  return (1-z) * z_prev\n",
    "\n",
    "def posedge(z, z_prev):\n",
    "  return z * (1 - z_prev)\n",
    "\n",
    "mpl.rcParams['ytick.left'] = True\n",
    "mpl.rcParams['ytick.labelleft'] = True\n",
    "mpl.rcParams['axes.spines.left'] = True\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "mpl.rcParams['axes.spines.bottom'] = True\n",
    "mpl.rcParams['legend.frameon'] = False\n",
    "\n",
    "def sampling_figure(ax, refrac):\n",
    "  T, _, _ = refrac.shape\n",
    "  N = 3\n",
    "\n",
    "  posedges = []\n",
    "  negedges = []\n",
    "\n",
    "  z_prev = torch.zeros_like(refrac[0])\n",
    "  z = refrac[0,:]\n",
    "\n",
    "  ax.set_yticks(1.2 * np.arange(0,5) + 0.6)\n",
    "  ax.set_yticklabels(['$z_1$', '$z_2$', '$z_3$', '$z_4$', '$z_5$'])\n",
    "  ax.spines['left'].set_visible(False)\n",
    "\n",
    "  for ts in range(T):\n",
    "    z = refrac[ts]\n",
    "    posedges.append(posedge(z, z_prev))\n",
    "    negedges.append(negedge(z, z_prev))\n",
    "    z_prev = z\n",
    "\n",
    "  posedges = torch.stack(posedges)\n",
    "  negedges = torch.stack(negedges)\n",
    "\n",
    "  lines = []\n",
    "  rects = []\n",
    "\n",
    "  facecolor = \"black\"\n",
    "\n",
    "  for k in range(N):\n",
    "      t = posedges[200:400,0,k].to_sparse().coalesce().indices()[0]\n",
    "      t_r = negedges[200:400,0,k].to_sparse().coalesce().indices()[0]\n",
    "\n",
    "\n",
    "      for ts, tf in zip(t, t_r):\n",
    "        rect = Rectangle((ts, k * 1.2), tf - ts, 1.0)\n",
    "        line = [(ts, k*1.2), (ts, k*1.2 + 1.0)]\n",
    "        rects.append(rect)\n",
    "        lines.append(line)\n",
    "\n",
    "  pc = PatchCollection(rects, facecolor=facecolor, alpha=0.2, edgecolor=None)\n",
    "  lc = LineCollection(lines, color=\"black\")\n",
    "\n",
    "  for k in range(N):\n",
    "    ax.plot(voltages[200:400,0,k].detach() + k * 1.2, label=f\"$v_{k}$\")\n",
    "\n",
    "\n",
    "  ax.add_collection(pc)\n",
    "  ax.set_ylabel(\"Neuron\")\n",
    "  ax.set_xlabel(\"T [ms]\")\n",
    "  # ax.add_collection(lc)\n",
    "  ax.legend(loc=\"upper right\", bbox_to_anchor=(1.15, 1))\n",
    "\n",
    "\n",
    "def figure_stochastic_computing(p_f, p_target, losses, refrac):\n",
    "  width = 0.35\n",
    "\n",
    "  fig = plt.figure(constrained_layout=True, figsize=(7, 9))\n",
    "  gs = fig.add_gridspec(nrows=3, ncols=1)\n",
    "\n",
    "  ax_sampling = fig.add_subplot(gs[0,0])\n",
    "  ax_prob = fig.add_subplot(gs[1,0])\n",
    "  ax_loss = fig.add_subplot(gs[2,0])\n",
    "\n",
    "  p_f_mean = np.mean(p_f.detach().numpy(),axis=0)\n",
    "  p_f_min = np.min(p_f.detach().numpy(), axis=0)\n",
    "  p_f_max = np.max(p_f.detach().numpy(), axis=0) \n",
    "\n",
    "  ax_sampling.text(-0.1, 1.1, \"A\", transform=ax_sampling.transAxes, size=\"medium\", weight=\"normal\")\n",
    "  sampling_figure(ax_sampling, refrac)\n",
    "\n",
    "  # plot the probabilities\n",
    "\n",
    "  ax_prob.text(-0.1, 1.1, \"B\", transform=ax_prob.transAxes, size='medium', weight='normal')\n",
    "  ax_prob.bar(np.arange(0,8)-width/2, p_f[0].detach().numpy(), width, label='$p_s$')\n",
    "  ax_prob.bar(np.arange(0,8)+width/2, p_target[0], width, label='$p_{t}$')\n",
    "  ax_prob.legend(loc='upper right')\n",
    "  ax_prob.set_xticks(np.arange(0,8))\n",
    "  ax_prob.set_xticklabels(['{0:03b}'.format(n) for n in np.arange(0,8)], rotation = 90)\n",
    "  ax_prob.set_ylabel('$p(z)$')\n",
    "  ax_prob.set_xlabel('$z$')\n",
    "  ax_prob.errorbar(np.arange(0,8)-width/2, p_f_mean, yerr=[p_f_mean - p_f_min, p_f_max - p_f_mean], color='r', fmt='.k')\n",
    "\n",
    "  ax_loss.text(-0.1, 1.1, \"C\", transform=ax_loss.transAxes, size='medium', weight='normal')\n",
    "  ax_loss.semilogy(losses)\n",
    "  ax_loss.set_ylabel('$KL(p_s | p_t)$')\n",
    "  ax_loss.set_xlabel('Epoch')\n",
    "  return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting plot shows part of the neuron membrane traces of the three readout neurons, the value of the binary refractory state variables is indicated by gray shading (A). We also see how well the sampled distribution approximates the target distribution (B). Finally we plot the Kulback-Leibler divergence (C)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = figure_stochastic_computing(p_f, p_target, losses, refrac)\n",
    "fig.savefig('stochastic_computing.png', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same kind of experiment can be repeated for 5 variables. Below\n",
    "is an example of one such run.\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/norse/norse-hbp-workshop/main/images/stochastic_computing.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
