{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7IYU0Bqomb2"
   },
   "source": [
    "# Single Neuron Experiments\n",
    "\n",
    "This tutorial introduces [Norse](norse.ai) and the concept of spiking neurons.\n",
    "In the next 5-10 minutes or so you will learn about\n",
    "\n",
    "- Spiking Neuron Models\n",
    "- Gradient based learning with Spiking Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Installation\n",
    "\n",
    "First of all, we will need to install Norse. Please run the cell below. Read on while it's running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet norse\n",
    "\n",
    "import torch\n",
    "import norse\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['ytick.left'] = True\n",
    "mpl.rcParams['ytick.labelleft'] = True\n",
    "mpl.rcParams['axes.spines.left'] = True\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "mpl.rcParams['axes.spines.bottom'] = True\n",
    "mpl.rcParams['legend.frameon'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: A simple neuron model\n",
    "\n",
    "The point neuron models supported by Norse are almost all variants of the Leaky-Integrate and Fire neuron model. It is however relatively easy to implement your own model. The library\n",
    "is build in layers, here I show an example of how to use the functional API directly. To\n",
    "build large scale machine learning models, you should check out the tutorial on [PyTorch\n",
    "lightning + Norse](high-performance-computing.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from norse.torch.functional.lif import lif_step, LIFParameters, LIFState\n",
    "\n",
    "class Neurons(torch.nn.Module):\n",
    "    def __init__(self, weights, alpha):\n",
    "        super(Neurons, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.w_in = torch.nn.Parameter(torch.tensor(weights).float())\n",
    "        self.w_rec = torch.zeros(hidden_size, hidden_size) # no recurrent connections\n",
    "        \n",
    "        self.lambda_vs = []\n",
    "        self.lambda_is = []\n",
    "        self.p = LIFParameters(alpha=alpha)\n",
    "\n",
    "    def reset_lambda_recording(self):\n",
    "        self.lambda_vs = []\n",
    "        self.lambda_is = []\n",
    "\n",
    "    def forward(self, z_in):\n",
    "        seq_length, batch_size, _ = z_in.shape\n",
    "        hidden_size = self.hidden_size\n",
    "\n",
    "        s = LIFState(\n",
    "            v = torch.zeros(batch_size, hidden_size, requires_grad=True),\n",
    "            i = torch.zeros(batch_size, hidden_size, requires_grad=True),\n",
    "            z = torch.zeros(batch_size, hidden_size)\n",
    "        )\n",
    "        voltages = torch.zeros(seq_length, batch_size, hidden_size)\n",
    "        currents = torch.zeros(seq_length, batch_size, hidden_size)\n",
    "        z_s = torch.zeros(seq_length, batch_size, hidden_size)\n",
    "\n",
    "        def save_lambda_v(grad):\n",
    "            if grad is not None:\n",
    "                self.lambda_vs.append(grad)\n",
    "\n",
    "        def save_lambda_i(grad):\n",
    "            if grad is not None:\n",
    "                self.lambda_is.append(grad)\n",
    "\n",
    "        for ts in range(seq_length):\n",
    "            z, s = lif_step(z_in[ts], s, self.w_in, self.w_rec, p=self.p)\n",
    "\n",
    "            # record the gradient in the backward pass\n",
    "            s.v.register_hook(save_lambda_v)\n",
    "            s.i.register_hook(save_lambda_i)\n",
    "\n",
    "            # save the voltage + synaptic input current state\n",
    "            voltages[ts,:] = s.v\n",
    "            currents[ts, :] = s.i\n",
    "            z_s[ts,:] = z\n",
    "\n",
    "        return z_s, voltages, currents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.1: Optimizing for a fixed number of spikes\n",
    "\n",
    "A simple task to consider is a single neuron stimulated at different times by $k$ fixed poisson distributed spike trains, with synaptic weights distributed according to a gaussian distribution. The goal is for the neuron to respond to these fixed spike trains with a certain number of spikes $n_\\text{target}$ within a time $T$. The loss in this case is\n",
    "$$\n",
    "l = -n_\\text{target}/T + \\sum_i \\delta(t - t_i) \n",
    "$$\n",
    "so\n",
    "$$\n",
    "S = \\int_0^T (-n_\\text{target}/T + \\sum_i \\delta(t - t_i)) dt = n_\\text{actual} - n_\\text{target}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange\n",
    "\n",
    "def run_training(\n",
    "    w_in,\n",
    "    z_in,\n",
    "    alpha=100.0,\n",
    "    max_epochs=100,\n",
    "    target_spikes=6,\n",
    "    target_spike_offset=10\n",
    "):\n",
    "    neurons = Neurons(w_in, alpha=torch.tensor(alpha))\n",
    "    optim = torch.optim.SGD(neurons.parameters(), lr=0.1)\n",
    "\n",
    "    lambda_vs = []\n",
    "    lambda_is = []\n",
    "    spikes_out = []\n",
    "    vs = []\n",
    "    cs = []\n",
    "\n",
    "    pbar = trange(max_epochs)\n",
    "    for e in pbar:\n",
    "        optim.zero_grad()\n",
    "        z_s, voltages, currents = neurons(z_in)\n",
    "\n",
    "        # compute the loss according to the formula above\n",
    "        loss = torch.sum(torch.abs((torch.sum(z_s, axis=0) - target_spikes)))\n",
    "        loss.backward()\n",
    "\n",
    "        pbar.set_postfix({\"spike difference\": loss.detach().item()})\n",
    "\n",
    "        # keep track of the experiment data\n",
    "        vs.append(voltages.detach().data)\n",
    "        cs.append(currents.detach().data)     \n",
    "        spikes_out.append(z_s.detach().data)\n",
    "        lambda_vs.append(torch.stack(neurons.lambda_vs))\n",
    "        lambda_is.append(torch.stack(neurons.lambda_is))\n",
    "        neurons.reset_lambda_recording()\n",
    "\n",
    "        if loss.data == torch.tensor([0.0]):\n",
    "            break\n",
    "\n",
    "        # do a gradient optimisation step\n",
    "        optim.step()\n",
    "\n",
    "    return spikes_out, vs, cs, lambda_vs, lambda_is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 1000\n",
    "input_size = 20\n",
    "hidden_size = 1\n",
    "batch_size = 1\n",
    "epochs = 100\n",
    "alpha = 100.0\n",
    "\n",
    "spikes = torch.distributions.bernoulli.Bernoulli(probs=0.04*torch.ones(seq_length, batch_size, input_size))\n",
    "z_in = spikes.sample()\n",
    "w_in = np.random.randn(hidden_size,input_size) * np.sqrt(2/hidden_size)\n",
    "spikes, vs, cs, lambda_vs, lambda_is = run_training(z_in=z_in, w_in=w_in, alpha=alpha, target_spikes=6, max_epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry that the progress bar turned red, in this case it means that the optimisation\n",
    "finished early. We can plot the error signals that are propagated backwards in time as follows. At each spike that reaches the neuron at synapse the variable $\\lambda_i$ is accumulated to the gradient\n",
    "of the synaptic weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epoch_from_last = 2\n",
    "plt.plot(lambda_vs[-epoch_from_last][:,0], label='$\\lambda_v$')\n",
    "plt.plot(lambda_is[-epoch_from_last][:,0], label='$\\lambda_i$')\n",
    "plt.xlabel('Time [ms]')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises:\n",
    "- Change the epoch_from_last variable to plot the error traces at different times in the optimisation\n",
    "  procedure.\n",
    "- Change the value alpha. What do you observe?\n",
    "- Repeat the experiment with more biologically realistic parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norse.torch.functional.lif.LIFParameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.2: Learning target spike times\n",
    "\n",
    "Another task is for one neuron to spike at specific spike times $t_0, \\ldots, t_N$ given that it stimulated \n",
    "by a fixed set of poisson distributed spikes. We can choose as a loss in this case\n",
    "$$\n",
    "l = \\sum_i \\lvert v - v_{\\text{th}} \\rvert^2 \\delta(t - t_i) + l_N\n",
    "$$\n",
    "that is we require the membrane voltages to be close to the threshold $v_{th}$ at the required spike times $t_i$\n",
    "and penalise the neuron if it spikes more or less than the required number of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange\n",
    "\n",
    "def run_target_spike_time_training(\n",
    "    w_in,\n",
    "    z_in,\n",
    "    alpha=100.0,\n",
    "    epochs=4000,\n",
    "    target_times=[100, 300, 500, 700]\n",
    "):\n",
    "    neurons = Neurons(w_in, alpha=torch.tensor(alpha))\n",
    "    optim = torch.optim.SGD(neurons.parameters(), lr=0.1)\n",
    "\n",
    "    lambda_vs = []\n",
    "    lambda_is = []\n",
    "    spikes_out = []\n",
    "    vs = []\n",
    "    cs = []\n",
    "    \n",
    "\n",
    "    v_target = torch.zeros(seq_length, batch_size, hidden_size)\n",
    "    target_spikes = len(target_times)\n",
    "\n",
    "    for time in target_times:  \n",
    "        v_target[time,:] = 1.1 * torch.ones(hidden_size)\n",
    "    \n",
    "    pbar = trange(epochs)\n",
    "    for e in pbar:\n",
    "        optim.zero_grad()\n",
    "\n",
    "        z_s, voltages, currents = neurons(z_in)\n",
    "        loss = torch.zeros(1,1)\n",
    "        for time in target_times:    \n",
    "            loss += 1/2 * 1/10 * (voltages[time,:] - v_target[time,:])**2\n",
    "\n",
    "        dspikes = torch.sum(torch.abs(torch.sum(z_s, axis=0) - target_spikes))\n",
    "        loss += dspikes\n",
    "        loss.backward()\n",
    "\n",
    "        pbar.set_postfix({\"loss\": loss.detach().item(), \"spike difference\": dspikes.detach().item()})\n",
    "\n",
    "        vs.append(voltages.detach().data)\n",
    "        cs.append(currents.detach().data)\n",
    "        spikes_out.append(z_s.detach().data)\n",
    "        lambda_vs.append(torch.stack(neurons.lambda_vs))\n",
    "        lambda_is.append(torch.stack(neurons.lambda_is))\n",
    "\n",
    "        neurons.lambda_vs = []\n",
    "        neurons.lambda_is = []\n",
    "        if loss.data == torch.tensor([0.0]):\n",
    "            break\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "    return spikes_out, vs, vs, lambda_vs, lambda_is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 1000\n",
    "input_size = 50\n",
    "hidden_size = 1\n",
    "batch_size = 1\n",
    "epochs = 100\n",
    "alpha = 100.0\n",
    "target_times = [100, 300, 500, 700]\n",
    "\n",
    "\n",
    "w_in = np.random.randn(hidden_size,input_size)* np.sqrt(2/hidden_size)\n",
    "spikes = torch.distributions.bernoulli.Bernoulli(probs=0.04*torch.ones(seq_length, batch_size, input_size))\n",
    "z_in = spikes.sample()\n",
    "result = run_target_spike_time_training(\n",
    "    w_in=w_in, \n",
    "    z_in=z_in,\n",
    "    alpha=alpha, \n",
    "    epochs=epochs, \n",
    "    target_times=target_times\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes, vs, cs, lambda_vs, lambda_is = result\n",
    "\n",
    "actual_times = spikes[-1][:,0,0].to_sparse().indices()[0]\n",
    "\n",
    "\n",
    "for ts in target_times:\n",
    "    plt.axvline(x=ts, color='red', linestyle='--')\n",
    "\n",
    "for ts in list(actual_times):\n",
    "    plt.axvline(x=ts, color='red', linestyle='-')\n",
    "\n",
    "plt.plot(vs[-1][:,0], label='$v$')\n",
    "plt.xlabel('Time [ms]')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again visualise the error traces over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lambda_vs[-2][:,0], label='$\\lambda_v$')\n",
    "plt.xlabel('Time [ms]')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises:\n",
    "- This task doesn't actually great, can you think of ways to improve it?\n",
    "- What additions to the loss could one consider to make the task more stable?\n",
    "- Explore different values for alpha, target_times and input size, what do you observe?\n",
    "- Consider a different optimiser\n",
    "- Consider using biologically plausible neuron parameters"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 1
}
