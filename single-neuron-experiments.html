
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Single Neuron Experiments &#8212; Norse Tutorial Notebook</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Norse Tutorial Notebook</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="README.html">
                    Norse Notebook Tutorials
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro_spikes.html">
   Spiking neurons in PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="intro_norse.html">
   Spiking neural networks with Norse
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="intro_plotting.html">
   Simulating and plotting neurons
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Supervised Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="mnist_classifiers.html">
   Training an MNIST classifier
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="poker-dvs_classifier.html">
   Training a classifier on the event-based POKER-DVS dataset
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Real-time event processing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="edge_detector.html">
   Detecting edges from event data with Norse
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Neuroscience
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="parameter-learning.html">
   Parameter learning in SNN with Norse
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="stp_example.html">
   Spike Time Dependent Plasticity
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Miscellaneous
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="high-performance-computing.html">
   High-Performance Computing with Norse and PyTorch Lightning
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/norse/notebooks"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/norse/notebooks/issues/new?title=Issue%20on%20page%20%2Fsingle-neuron-experiments.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/single-neuron-experiments.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#step-0-installation">
   Step 0: Installation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#step-1-a-simple-neuron-model">
   Step 1: A simple neuron model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#step-2-1-optimizing-for-a-fixed-number-of-spikes">
   Step 2.1: Optimizing for a fixed number of spikes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#step-2-2-learning-target-spike-times">
   Step 2.2: Learning target spike times
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Single Neuron Experiments</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#step-0-installation">
   Step 0: Installation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#step-1-a-simple-neuron-model">
   Step 1: A simple neuron model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#step-2-1-optimizing-for-a-fixed-number-of-spikes">
   Step 2.1: Optimizing for a fixed number of spikes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#step-2-2-learning-target-spike-times">
   Step 2.2: Learning target spike times
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <!-- #region id="b7IYU0Bqomb2" -->
<section class="tex2jax_ignore mathjax_ignore" id="single-neuron-experiments">
<h1>Single Neuron Experiments<a class="headerlink" href="#single-neuron-experiments" title="Permalink to this headline">#</a></h1>
<p>This tutorial introduces <span class="xref myst">Norse</span> and the concept of spiking neurons.
In the next 5-10 minutes or so you will learn about</p>
<ul class="simple">
<li><p>Spiking Neuron Models</p></li>
<li><p>Gradient based learning with Spiking Neurons</p></li>
</ul>
<!-- #endregion -->
<section id="step-0-installation">
<h2>Step 0: Installation<a class="headerlink" href="#step-0-installation" title="Permalink to this headline">#</a></h2>
<p>First of all, we will need to install Norse. Please run the cell below. Read on while it’s running.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>!pip install --quiet norse

import torch
import norse
import numpy as np
import matplotlib as mpl

mpl.rcParams[&quot;ytick.left&quot;] = True
mpl.rcParams[&quot;ytick.labelleft&quot;] = True
mpl.rcParams[&quot;axes.spines.left&quot;] = True
mpl.rcParams[&quot;axes.spines.right&quot;] = False
mpl.rcParams[&quot;axes.spines.top&quot;] = False
mpl.rcParams[&quot;axes.spines.bottom&quot;] = True
mpl.rcParams[&quot;legend.frameon&quot;] = False
</pre></div>
</div>
</section>
<section id="step-1-a-simple-neuron-model">
<h2>Step 1: A simple neuron model<a class="headerlink" href="#step-1-a-simple-neuron-model" title="Permalink to this headline">#</a></h2>
<p>The point neuron models supported by Norse are almost all variants of the Leaky-Integrate and Fire neuron model. It is however relatively easy to implement your own model. The library
is build in layers, here I show an example of how to use the functional API directly. To
build large scale machine learning models, you should check out the tutorial on <a class="reference internal" href="high-performance-computing.html"><span class="doc std std-doc">PyTorch
lightning + Norse</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">norse.torch.functional.lif</span> <span class="kn">import</span> <span class="n">lif_step</span><span class="p">,</span> <span class="n">LIFParameters</span><span class="p">,</span> <span class="n">LIFState</span>


<span class="k">class</span> <span class="nc">Neurons</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Neurons</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w_in</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w_rec</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>  <span class="c1"># no recurrent connections</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">lambda_vs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lambda_is</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">LIFParameters</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reset_lambda_recording</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lambda_vs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lambda_is</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z_in</span><span class="p">):</span>
        <span class="n">seq_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">z_in</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">hidden_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span>

        <span class="n">s</span> <span class="o">=</span> <span class="n">LIFState</span><span class="p">(</span>
            <span class="n">v</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">i</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">z</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="n">voltages</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">currents</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">z_s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">save_lambda_v</span><span class="p">(</span><span class="n">grad</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">lambda_vs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">save_lambda_i</span><span class="p">(</span><span class="n">grad</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">lambda_is</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">ts</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_length</span><span class="p">):</span>
            <span class="n">z</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="n">lif_step</span><span class="p">(</span><span class="n">z_in</span><span class="p">[</span><span class="n">ts</span><span class="p">],</span> <span class="n">s</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_in</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_rec</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">)</span>

            <span class="c1"># record the gradient in the backward pass</span>
            <span class="n">s</span><span class="o">.</span><span class="n">v</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="n">save_lambda_v</span><span class="p">)</span>
            <span class="n">s</span><span class="o">.</span><span class="n">i</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="n">save_lambda_i</span><span class="p">)</span>

            <span class="c1"># save the voltage + synaptic input current state</span>
            <span class="n">voltages</span><span class="p">[</span><span class="n">ts</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">v</span>
            <span class="n">currents</span><span class="p">[</span><span class="n">ts</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">i</span>
            <span class="n">z_s</span><span class="p">[</span><span class="n">ts</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">z</span>

        <span class="k">return</span> <span class="n">z_s</span><span class="p">,</span> <span class="n">voltages</span><span class="p">,</span> <span class="n">currents</span>
</pre></div>
</div>
</section>
<section id="step-2-1-optimizing-for-a-fixed-number-of-spikes">
<h2>Step 2.1: Optimizing for a fixed number of spikes<a class="headerlink" href="#step-2-1-optimizing-for-a-fixed-number-of-spikes" title="Permalink to this headline">#</a></h2>
<p>A simple task to consider is a single neuron stimulated at different times by <span class="math notranslate nohighlight">\(k\)</span> fixed poisson distributed spike trains, with synaptic weights distributed according to a gaussian distribution. The goal is for the neuron to respond to these fixed spike trains with a certain number of spikes <span class="math notranslate nohighlight">\(n_\text{target}\)</span> within a time <span class="math notranslate nohighlight">\(T\)</span>. The loss in this case is
$<span class="math notranslate nohighlight">\(
l = -n_\text{target}/T + \sum_i \delta(t - t_i)
\)</span><span class="math notranslate nohighlight">\(
so
\)</span><span class="math notranslate nohighlight">\(
S = \int_0^T (-n_\text{target}/T + \sum_i \delta(t - t_i)) dt = n_\text{actual} - n_\text{target}
\)</span>$</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tqdm.notebook</span> <span class="kn">import</span> <span class="n">trange</span>


<span class="k">def</span> <span class="nf">run_training</span><span class="p">(</span>
    <span class="n">w_in</span><span class="p">,</span> <span class="n">z_in</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">100.0</span><span class="p">,</span> <span class="n">max_epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">target_spikes</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">target_spike_offset</span><span class="o">=</span><span class="mi">10</span>
<span class="p">):</span>
    <span class="n">neurons</span> <span class="o">=</span> <span class="n">Neurons</span><span class="p">(</span><span class="n">w_in</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">alpha</span><span class="p">))</span>
    <span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">neurons</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

    <span class="n">lambda_vs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">lambda_is</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">spikes_out</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">vs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">cs</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">pbar</span> <span class="o">=</span> <span class="n">trange</span><span class="p">(</span><span class="n">max_epochs</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">pbar</span><span class="p">:</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">z_s</span><span class="p">,</span> <span class="n">voltages</span><span class="p">,</span> <span class="n">currents</span> <span class="o">=</span> <span class="n">neurons</span><span class="p">(</span><span class="n">z_in</span><span class="p">)</span>

        <span class="c1"># compute the loss according to the formula above</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">z_s</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="n">target_spikes</span><span class="p">)))</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="n">pbar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s2">&quot;spike difference&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()})</span>

        <span class="c1"># keep track of the experiment data</span>
        <span class="n">vs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">voltages</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="n">cs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">currents</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="n">spikes_out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">z_s</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="n">lambda_vs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">neurons</span><span class="o">.</span><span class="n">lambda_vs</span><span class="p">))</span>
        <span class="n">lambda_is</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">neurons</span><span class="o">.</span><span class="n">lambda_is</span><span class="p">))</span>
        <span class="n">neurons</span><span class="o">.</span><span class="n">reset_lambda_recording</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">loss</span><span class="o">.</span><span class="n">data</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">]):</span>
            <span class="k">break</span>

        <span class="c1"># do a gradient optimisation step</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">spikes_out</span><span class="p">,</span> <span class="n">vs</span><span class="p">,</span> <span class="n">cs</span><span class="p">,</span> <span class="n">lambda_vs</span><span class="p">,</span> <span class="n">lambda_is</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">seq_length</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">input_size</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">100.0</span>

<span class="n">spikes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">bernoulli</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span>
    <span class="n">probs</span><span class="o">=</span><span class="mf">0.04</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">z_in</span> <span class="o">=</span> <span class="n">spikes</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
<span class="n">w_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">hidden_size</span><span class="p">)</span>
<span class="n">spikes</span><span class="p">,</span> <span class="n">vs</span><span class="p">,</span> <span class="n">cs</span><span class="p">,</span> <span class="n">lambda_vs</span><span class="p">,</span> <span class="n">lambda_is</span> <span class="o">=</span> <span class="n">run_training</span><span class="p">(</span>
    <span class="n">z_in</span><span class="o">=</span><span class="n">z_in</span><span class="p">,</span> <span class="n">w_in</span><span class="o">=</span><span class="n">w_in</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">target_spikes</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">max_epochs</span><span class="o">=</span><span class="n">epochs</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Don’t worry that the progress bar turned red, in this case it means that the optimisation
finished early. We can plot the error signals that are propagated backwards in time as follows. At each spike that reaches the neuron at synapse the variable <span class="math notranslate nohighlight">\(\lambda_i\)</span> is accumulated to the gradient
of the synaptic weight.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">epoch_from_last</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lambda_vs</span><span class="p">[</span><span class="o">-</span><span class="n">epoch_from_last</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$\lambda_v$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lambda_is</span><span class="p">[</span><span class="o">-</span><span class="n">epoch_from_last</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$\lambda_i$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Time [ms]&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
<p>Exercises:</p>
<ul class="simple">
<li><p>Change the epoch_from_last variable to plot the error traces at different times in the optimisation
procedure.</p></li>
<li><p>Change the value alpha. What do you observe?</p></li>
<li><p>Repeat the experiment with more biologically realistic parameters</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>norse.torch.functional.lif.LIFParameters?
</pre></div>
</div>
</section>
<section id="step-2-2-learning-target-spike-times">
<h2>Step 2.2: Learning target spike times<a class="headerlink" href="#step-2-2-learning-target-spike-times" title="Permalink to this headline">#</a></h2>
<p>Another task is for one neuron to spike at specific spike times <span class="math notranslate nohighlight">\(t_0, \ldots, t_N\)</span> given that it stimulated
by a fixed set of poisson distributed spikes. We can choose as a loss in this case
$<span class="math notranslate nohighlight">\(
l = \sum_i \lvert v - v_{\text{th}} \rvert^2 \delta(t - t_i) + l_N
\)</span><span class="math notranslate nohighlight">\(
that is we require the membrane voltages to be close to the threshold \)</span>v_{th}<span class="math notranslate nohighlight">\( at the required spike times \)</span>t_i$
and penalise the neuron if it spikes more or less than the required number of times.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tqdm.notebook</span> <span class="kn">import</span> <span class="n">trange</span>


<span class="k">def</span> <span class="nf">run_target_spike_time_training</span><span class="p">(</span>
    <span class="n">w_in</span><span class="p">,</span> <span class="n">z_in</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">100.0</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span> <span class="n">target_times</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">700</span><span class="p">]</span>
<span class="p">):</span>
    <span class="n">neurons</span> <span class="o">=</span> <span class="n">Neurons</span><span class="p">(</span><span class="n">w_in</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">alpha</span><span class="p">))</span>
    <span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">neurons</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

    <span class="n">lambda_vs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">lambda_is</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">spikes_out</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">vs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">cs</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">v_target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
    <span class="n">target_spikes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">target_times</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">time</span> <span class="ow">in</span> <span class="n">target_times</span><span class="p">:</span>
        <span class="n">v_target</span><span class="p">[</span><span class="n">time</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mf">1.1</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span>

    <span class="n">pbar</span> <span class="o">=</span> <span class="n">trange</span><span class="p">(</span><span class="n">epochs</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">pbar</span><span class="p">:</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="n">z_s</span><span class="p">,</span> <span class="n">voltages</span><span class="p">,</span> <span class="n">currents</span> <span class="o">=</span> <span class="n">neurons</span><span class="p">(</span><span class="n">z_in</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">time</span> <span class="ow">in</span> <span class="n">target_times</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">*</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">10</span> <span class="o">*</span> <span class="p">(</span><span class="n">voltages</span><span class="p">[</span><span class="n">time</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-</span> <span class="n">v_target</span><span class="p">[</span><span class="n">time</span><span class="p">,</span> <span class="p">:])</span> <span class="o">**</span> <span class="mi">2</span>

        <span class="n">dspikes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">z_s</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="n">target_spikes</span><span class="p">))</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="n">dspikes</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="n">pbar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">(</span>
            <span class="p">{</span><span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="s2">&quot;spike difference&quot;</span><span class="p">:</span> <span class="n">dspikes</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()}</span>
        <span class="p">)</span>

        <span class="n">vs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">voltages</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="n">cs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">currents</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="n">spikes_out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">z_s</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="n">lambda_vs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">neurons</span><span class="o">.</span><span class="n">lambda_vs</span><span class="p">))</span>
        <span class="n">lambda_is</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">neurons</span><span class="o">.</span><span class="n">lambda_is</span><span class="p">))</span>

        <span class="n">neurons</span><span class="o">.</span><span class="n">lambda_vs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">neurons</span><span class="o">.</span><span class="n">lambda_is</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">loss</span><span class="o">.</span><span class="n">data</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">]):</span>
            <span class="k">break</span>

        <span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">spikes_out</span><span class="p">,</span> <span class="n">vs</span><span class="p">,</span> <span class="n">vs</span><span class="p">,</span> <span class="n">lambda_vs</span><span class="p">,</span> <span class="n">lambda_is</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">seq_length</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">input_size</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">100.0</span>
<span class="n">target_times</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">700</span><span class="p">]</span>


<span class="n">w_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">hidden_size</span><span class="p">)</span>
<span class="n">spikes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">bernoulli</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span>
    <span class="n">probs</span><span class="o">=</span><span class="mf">0.04</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">z_in</span> <span class="o">=</span> <span class="n">spikes</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">run_target_spike_time_training</span><span class="p">(</span>
    <span class="n">w_in</span><span class="o">=</span><span class="n">w_in</span><span class="p">,</span> <span class="n">z_in</span><span class="o">=</span><span class="n">z_in</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span> <span class="n">target_times</span><span class="o">=</span><span class="n">target_times</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">spikes</span><span class="p">,</span> <span class="n">vs</span><span class="p">,</span> <span class="n">cs</span><span class="p">,</span> <span class="n">lambda_vs</span><span class="p">,</span> <span class="n">lambda_is</span> <span class="o">=</span> <span class="n">result</span>

<span class="n">actual_times</span> <span class="o">=</span> <span class="n">spikes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">()</span><span class="o">.</span><span class="n">indices</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>


<span class="k">for</span> <span class="n">ts</span> <span class="ow">in</span> <span class="n">target_times</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">ts</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">ts</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">actual_times</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">ts</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;-&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">vs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$v$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Time [ms]&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
<p>We again visualise the error traces over time.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lambda_vs</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$\lambda_v$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Time [ms]&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
<p>Exercises:</p>
<ul class="simple">
<li><p>This task doesn’t actually great, can you think of ways to improve it?</p></li>
<li><p>What additions to the loss could one consider to make the task more stable?</p></li>
<li><p>Explore different values for alpha, target_times and input size, what do you observe?</p></li>
<li><p>Consider a different optimiser</p></li>
<li><p>Consider using biologically plausible neuron parameters</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "norse/notebooks",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Norse authors<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>