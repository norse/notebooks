
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Single Neuron Experiments &#8212; Norse Tutorial Notebook</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Norse Tutorial Notebook</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="README.html">
   Norse Notebook Tutorials
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro_spikes.html">
   Spiking neurons in PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="intro_norse.html">
   Spiking neural networks with Norse
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="intro_plotting.html">
   Simulating and plotting neurons
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Supervised Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="mnist_classifiers.html">
   Training an MNIST classifier
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="poker-dvs_classifier.html">
   Training a classifier on the event-based POKER-DVS dataset
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Neuroscience
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="parameter-learning.html">
   Parameter learning in SNN with Norse
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="stp_example.html">
   Spike Time Dependent Plasticity
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Miscellaneous
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="high-performance-computing.html">
   High-Performance Computing with Norse and PyTorch Lightning
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/single-neuron-experiments.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/norse/notebooks"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/norse/notebooks/issues/new?title=Issue%20on%20page%20%2Fsingle-neuron-experiments.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#step-0-installation">
   Step 0: Installation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#step-1-a-simple-neuron-model">
   Step 1: A simple neuron model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#step-2-1-optimizing-for-a-fixed-number-of-spikes">
   Step 2.1: Optimizing for a fixed number of spikes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#step-2-2-learning-target-spike-times">
   Step 2.2: Learning target spike times
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Single Neuron Experiments</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#step-0-installation">
   Step 0: Installation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#step-1-a-simple-neuron-model">
   Step 1: A simple neuron model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#step-2-1-optimizing-for-a-fixed-number-of-spikes">
   Step 2.1: Optimizing for a fixed number of spikes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#step-2-2-learning-target-spike-times">
   Step 2.2: Learning target spike times
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="single-neuron-experiments">
<h1>Single Neuron Experiments<a class="headerlink" href="#single-neuron-experiments" title="Permalink to this headline">¶</a></h1>
<p>This tutorial introduces <span class="xref myst">Norse</span> and the concept of spiking neurons.
In the next 5-10 minutes or so you will learn about</p>
<ul class="simple">
<li><p>Spiking Neuron Models</p></li>
<li><p>Gradient based learning with Spiking Neurons</p></li>
</ul>
<div class="section" id="step-0-installation">
<h2>Step 0: Installation<a class="headerlink" href="#step-0-installation" title="Permalink to this headline">¶</a></h2>
<p>First of all, we will need to install Norse. Please run the cell below. Read on while it’s running.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!pip install --quiet norse

import torch
import norse
import numpy as np
import matplotlib as mpl

mpl.rcParams[&#39;ytick.left&#39;] = True
mpl.rcParams[&#39;ytick.labelleft&#39;] = True
mpl.rcParams[&#39;axes.spines.left&#39;] = True
mpl.rcParams[&#39;axes.spines.right&#39;] = False
mpl.rcParams[&#39;axes.spines.top&#39;] = False
mpl.rcParams[&#39;axes.spines.bottom&#39;] = True
mpl.rcParams[&#39;legend.frameon&#39;] = False
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="step-1-a-simple-neuron-model">
<h2>Step 1: A simple neuron model<a class="headerlink" href="#step-1-a-simple-neuron-model" title="Permalink to this headline">¶</a></h2>
<p>The point neuron models supported by Norse are almost all variants of the Leaky-Integrate and Fire neuron model. It is however relatively easy to implement your own model. The library
is build in layers, here I show an example of how to use the functional API directly. To
build large scale machine learning models, you should check out the tutorial on <a class="reference internal" href="high-performance-computing.html"><span class="doc std std-doc">PyTorch
lightning + Norse</span></a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">norse.torch.functional.lif</span> <span class="kn">import</span> <span class="n">lif_step</span><span class="p">,</span> <span class="n">LIFParameters</span><span class="p">,</span> <span class="n">LIFState</span>

<span class="k">class</span> <span class="nc">Neurons</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Neurons</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w_in</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w_rec</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span> <span class="c1"># no recurrent connections</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">lambda_vs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lambda_is</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">LIFParameters</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reset_lambda_recording</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lambda_vs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lambda_is</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z_in</span><span class="p">):</span>
        <span class="n">seq_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">z_in</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">hidden_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span>

        <span class="n">s</span> <span class="o">=</span> <span class="n">LIFState</span><span class="p">(</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">voltages</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">currents</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">z_s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">save_lambda_v</span><span class="p">(</span><span class="n">grad</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">lambda_vs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">save_lambda_i</span><span class="p">(</span><span class="n">grad</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">lambda_is</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">ts</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_length</span><span class="p">):</span>
            <span class="n">z</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="n">lif_step</span><span class="p">(</span><span class="n">z_in</span><span class="p">[</span><span class="n">ts</span><span class="p">],</span> <span class="n">s</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_in</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_rec</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">)</span>

            <span class="c1"># record the gradient in the backward pass</span>
            <span class="n">s</span><span class="o">.</span><span class="n">v</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="n">save_lambda_v</span><span class="p">)</span>
            <span class="n">s</span><span class="o">.</span><span class="n">i</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="n">save_lambda_i</span><span class="p">)</span>

            <span class="c1"># save the voltage + synaptic input current state</span>
            <span class="n">voltages</span><span class="p">[</span><span class="n">ts</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">v</span>
            <span class="n">currents</span><span class="p">[</span><span class="n">ts</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">i</span>
            <span class="n">z_s</span><span class="p">[</span><span class="n">ts</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">z</span>

        <span class="k">return</span> <span class="n">z_s</span><span class="p">,</span> <span class="n">voltages</span><span class="p">,</span> <span class="n">currents</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="step-2-1-optimizing-for-a-fixed-number-of-spikes">
<h2>Step 2.1: Optimizing for a fixed number of spikes<a class="headerlink" href="#step-2-1-optimizing-for-a-fixed-number-of-spikes" title="Permalink to this headline">¶</a></h2>
<p>A simple task to consider is a single neuron stimulated at different times by <span class="math notranslate nohighlight">\(k\)</span> fixed poisson distributed spike trains, with synaptic weights distributed according to a gaussian distribution. The goal is for the neuron to respond to these fixed spike trains with a certain number of spikes <span class="math notranslate nohighlight">\(n_\text{target}\)</span> within a time <span class="math notranslate nohighlight">\(T\)</span>. The loss in this case is
$<span class="math notranslate nohighlight">\(
l = -n_\text{target}/T + \sum_i \delta(t - t_i) 
\)</span><span class="math notranslate nohighlight">\(
so
\)</span><span class="math notranslate nohighlight">\(
S = \int_0^T (-n_\text{target}/T + \sum_i \delta(t - t_i)) dt = n_\text{actual} - n_\text{target}
\)</span>$</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tqdm.notebook</span> <span class="kn">import</span> <span class="n">trange</span>

<span class="k">def</span> <span class="nf">run_training</span><span class="p">(</span>
    <span class="n">w_in</span><span class="p">,</span>
    <span class="n">z_in</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">100.0</span><span class="p">,</span>
    <span class="n">max_epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">target_spikes</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
    <span class="n">target_spike_offset</span><span class="o">=</span><span class="mi">10</span>
<span class="p">):</span>
    <span class="n">neurons</span> <span class="o">=</span> <span class="n">Neurons</span><span class="p">(</span><span class="n">w_in</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">alpha</span><span class="p">))</span>
    <span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">neurons</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

    <span class="n">lambda_vs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">lambda_is</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">spikes_out</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">vs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">cs</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">pbar</span> <span class="o">=</span> <span class="n">trange</span><span class="p">(</span><span class="n">max_epochs</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">pbar</span><span class="p">:</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">z_s</span><span class="p">,</span> <span class="n">voltages</span><span class="p">,</span> <span class="n">currents</span> <span class="o">=</span> <span class="n">neurons</span><span class="p">(</span><span class="n">z_in</span><span class="p">)</span>

        <span class="c1"># compute the loss according to the formula above</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">z_s</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="n">target_spikes</span><span class="p">)))</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="n">pbar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s2">&quot;spike difference&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()})</span>

        <span class="c1"># keep track of the experiment data</span>
        <span class="n">vs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">voltages</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="n">cs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">currents</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>     
        <span class="n">spikes_out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">z_s</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="n">lambda_vs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">neurons</span><span class="o">.</span><span class="n">lambda_vs</span><span class="p">))</span>
        <span class="n">lambda_is</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">neurons</span><span class="o">.</span><span class="n">lambda_is</span><span class="p">))</span>
        <span class="n">neurons</span><span class="o">.</span><span class="n">reset_lambda_recording</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">loss</span><span class="o">.</span><span class="n">data</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">]):</span>
            <span class="k">break</span>

        <span class="c1"># do a gradient optimisation step</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">spikes_out</span><span class="p">,</span> <span class="n">vs</span><span class="p">,</span> <span class="n">cs</span><span class="p">,</span> <span class="n">lambda_vs</span><span class="p">,</span> <span class="n">lambda_is</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">seq_length</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">input_size</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">100.0</span>

<span class="n">spikes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">bernoulli</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="mf">0.04</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">))</span>
<span class="n">z_in</span> <span class="o">=</span> <span class="n">spikes</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
<span class="n">w_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span><span class="n">input_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">hidden_size</span><span class="p">)</span>
<span class="n">spikes</span><span class="p">,</span> <span class="n">vs</span><span class="p">,</span> <span class="n">cs</span><span class="p">,</span> <span class="n">lambda_vs</span><span class="p">,</span> <span class="n">lambda_is</span> <span class="o">=</span> <span class="n">run_training</span><span class="p">(</span><span class="n">z_in</span><span class="o">=</span><span class="n">z_in</span><span class="p">,</span> <span class="n">w_in</span><span class="o">=</span><span class="n">w_in</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">target_spikes</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">max_epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Don’t worry that the progress bar turned red, in this case it means that the optimisation
finished early. We can plot the error signals that are propagated backwards in time as follows. At each spike that reaches the neuron at synapse the variable <span class="math notranslate nohighlight">\(\lambda_i\)</span> is accumulated to the gradient
of the synaptic weight.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">epoch_from_last</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lambda_vs</span><span class="p">[</span><span class="o">-</span><span class="n">epoch_from_last</span><span class="p">][:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$\lambda_v$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lambda_is</span><span class="p">[</span><span class="o">-</span><span class="n">epoch_from_last</span><span class="p">][:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$\lambda_i$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time [ms]&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Exercises:</p>
<ul class="simple">
<li><p>Change the epoch_from_last variable to plot the error traces at different times in the optimisation
procedure.</p></li>
<li><p>Change the value alpha. What do you observe?</p></li>
<li><p>Repeat the experiment with more biologically realistic parameters</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>norse.torch.functional.lif.LIFParameters?
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="step-2-2-learning-target-spike-times">
<h2>Step 2.2: Learning target spike times<a class="headerlink" href="#step-2-2-learning-target-spike-times" title="Permalink to this headline">¶</a></h2>
<p>Another task is for one neuron to spike at specific spike times <span class="math notranslate nohighlight">\(t_0, \ldots, t_N\)</span> given that it stimulated
by a fixed set of poisson distributed spikes. We can choose as a loss in this case
$<span class="math notranslate nohighlight">\(
l = \sum_i \lvert v - v_{\text{th}} \rvert^2 \delta(t - t_i) + l_N
\)</span><span class="math notranslate nohighlight">\(
that is we require the membrane voltages to be close to the threshold \)</span>v_{th}<span class="math notranslate nohighlight">\( at the required spike times \)</span>t_i$
and penalise the neuron if it spikes more or less than the required number of times.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tqdm.notebook</span> <span class="kn">import</span> <span class="n">trange</span>

<span class="k">def</span> <span class="nf">run_target_spike_time_training</span><span class="p">(</span>
    <span class="n">w_in</span><span class="p">,</span>
    <span class="n">z_in</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">100.0</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span>
    <span class="n">target_times</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">700</span><span class="p">]</span>
<span class="p">):</span>
    <span class="n">neurons</span> <span class="o">=</span> <span class="n">Neurons</span><span class="p">(</span><span class="n">w_in</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">alpha</span><span class="p">))</span>
    <span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">neurons</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

    <span class="n">lambda_vs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">lambda_is</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">spikes_out</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">vs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">cs</span> <span class="o">=</span> <span class="p">[]</span>
    

    <span class="n">v_target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
    <span class="n">target_spikes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">target_times</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">time</span> <span class="ow">in</span> <span class="n">target_times</span><span class="p">:</span>  
        <span class="n">v_target</span><span class="p">[</span><span class="n">time</span><span class="p">,:]</span> <span class="o">=</span> <span class="mf">1.1</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span>
    
    <span class="n">pbar</span> <span class="o">=</span> <span class="n">trange</span><span class="p">(</span><span class="n">epochs</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">pbar</span><span class="p">:</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="n">z_s</span><span class="p">,</span> <span class="n">voltages</span><span class="p">,</span> <span class="n">currents</span> <span class="o">=</span> <span class="n">neurons</span><span class="p">(</span><span class="n">z_in</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">time</span> <span class="ow">in</span> <span class="n">target_times</span><span class="p">:</span>    
            <span class="n">loss</span> <span class="o">+=</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span> <span class="o">*</span> <span class="mi">1</span><span class="o">/</span><span class="mi">10</span> <span class="o">*</span> <span class="p">(</span><span class="n">voltages</span><span class="p">[</span><span class="n">time</span><span class="p">,:]</span> <span class="o">-</span> <span class="n">v_target</span><span class="p">[</span><span class="n">time</span><span class="p">,:])</span><span class="o">**</span><span class="mi">2</span>

        <span class="n">dspikes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">z_s</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="n">target_spikes</span><span class="p">))</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="n">dspikes</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="n">pbar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="s2">&quot;spike difference&quot;</span><span class="p">:</span> <span class="n">dspikes</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()})</span>

        <span class="n">vs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">voltages</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="n">cs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">currents</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="n">spikes_out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">z_s</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="n">lambda_vs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">neurons</span><span class="o">.</span><span class="n">lambda_vs</span><span class="p">))</span>
        <span class="n">lambda_is</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">neurons</span><span class="o">.</span><span class="n">lambda_is</span><span class="p">))</span>

        <span class="n">neurons</span><span class="o">.</span><span class="n">lambda_vs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">neurons</span><span class="o">.</span><span class="n">lambda_is</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">loss</span><span class="o">.</span><span class="n">data</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">]):</span>
            <span class="k">break</span>

        <span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">spikes_out</span><span class="p">,</span> <span class="n">vs</span><span class="p">,</span> <span class="n">vs</span><span class="p">,</span> <span class="n">lambda_vs</span><span class="p">,</span> <span class="n">lambda_is</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">seq_length</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">input_size</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">100.0</span>
<span class="n">target_times</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">700</span><span class="p">]</span>


<span class="n">w_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span><span class="n">input_size</span><span class="p">)</span><span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">hidden_size</span><span class="p">)</span>
<span class="n">spikes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">bernoulli</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="mf">0.04</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">))</span>
<span class="n">z_in</span> <span class="o">=</span> <span class="n">spikes</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">run_target_spike_time_training</span><span class="p">(</span>
    <span class="n">w_in</span><span class="o">=</span><span class="n">w_in</span><span class="p">,</span> 
    <span class="n">z_in</span><span class="o">=</span><span class="n">z_in</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> 
    <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span> 
    <span class="n">target_times</span><span class="o">=</span><span class="n">target_times</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">spikes</span><span class="p">,</span> <span class="n">vs</span><span class="p">,</span> <span class="n">cs</span><span class="p">,</span> <span class="n">lambda_vs</span><span class="p">,</span> <span class="n">lambda_is</span> <span class="o">=</span> <span class="n">result</span>

<span class="n">actual_times</span> <span class="o">=</span> <span class="n">spikes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][:,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to_sparse</span><span class="p">()</span><span class="o">.</span><span class="n">indices</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>


<span class="k">for</span> <span class="n">ts</span> <span class="ow">in</span> <span class="n">target_times</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">ts</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">ts</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">actual_times</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">ts</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">vs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$v$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time [ms]&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>We again visualise the error traces over time.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lambda_vs</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">][:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$\lambda_v$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time [ms]&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Exercises:</p>
<ul class="simple">
<li><p>This task doesn’t actually great, can you think of ways to improve it?</p></li>
<li><p>What additions to the loss could one consider to make the task more stable?</p></li>
<li><p>Explore different values for alpha, target_times and input size, what do you observe?</p></li>
<li><p>Consider a different optimiser</p></li>
<li><p>Consider using biologically plausible neuron parameters</p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "norse/notebooks",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Norse authors<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>